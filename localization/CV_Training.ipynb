{"cells":[{"cell_type":"markdown","metadata":{"id":"53rFv2JeGURz"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"16Oo66Xfn3jg"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"7U_KP4n4nQ8W"},"source":["Author: Biffis Nicola.\n","\n","With this Python script we will train a neural network, starting from a pre-trained one, using the Tensorflow and KerasCV libraries"]},{"cell_type":"markdown","metadata":{"id":"69mlRPmGn_gZ"},"source":["## Work environment preparation"]},{"cell_type":"markdown","metadata":{"id":"gxrQQm3apebR"},"source":["KerasCV package installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klYQvb5y4nEN"},"outputs":[],"source":["!pip install --upgrade -q git+https://github.com/keras-team/keras-cv"]},{"cell_type":"markdown","metadata":{"id":"H2MudR98zJuX"},"source":["Importing the necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0vskI6C4ov6"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import optimizers\n","import keras_cv\n","import numpy as np\n","from keras_cv import bounding_box\n","import os\n","import resource\n","from keras_cv import visualization\n","import tqdm"]},{"cell_type":"markdown","metadata":{"id":"vHWFaBDNmyGV"},"source":["Enable the use of the GPU to reduce the training time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PA3zCAomX5V"},"outputs":[],"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"markdown","metadata":{"id":"l-hceWMIpoqB"},"source":["Connection to google drive to save the created model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sdh9sXTn4rBP"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695020803774,"user":{"displayName":"Nicola Biffis","userId":"08845686113978860775"},"user_tz":-120},"id":"nr68uOrl4sTu","outputId":"46ac3eb0-3681-4ef2-d700-ab9429a4da3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks\n"]}],"source":["cd drive/MyDrive/Colab\\ Notebooks"]},{"cell_type":"markdown","metadata":{"id":"P10Vl6c4pxPz"},"source":["## Definition of useful variables"]},{"cell_type":"markdown","metadata":{"id":"oY77n4wMqBNH"},"source":["Definition of the possible classes present in the dataset that we will use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPvV5jK45HBW"},"outputs":[],"source":["class_ids = [\n","    \"Aeroplane\",\n","    \"Bicycle\",\n","    \"Bird\",\n","    \"Boat\",\n","    \"Bottle\",\n","    \"Bus\",\n","    \"Car\",\n","    \"Cat\",\n","    \"Chair\",\n","    \"Cow\",\n","    \"Dining Table\",\n","    \"Dog\",\n","    \"Horse\",\n","    \"Motorbike\",\n","    \"Person\",\n","    \"Potted Plant\",\n","    \"Sheep\",\n","    \"Sofa\",\n","    \"Train\",\n","    \"Tvmonitor\",\n","    \"Total\",\n","]\n","class_mapping = dict(zip(range(len(class_ids)), class_ids))"]},{"cell_type":"markdown","metadata":{"id":"yTUlG4n1qOEF"},"source":["Definition of the batch size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaBxRQbj5KOz"},"outputs":[],"source":["BATCH_SIZE = 4"]},{"cell_type":"markdown","metadata":{"id":"TnqXk7tzrMGU"},"source":["## Data loading"]},{"cell_type":"markdown","metadata":{"id":"OgUwUO2qqNE1"},"source":["Definition of the functions to load the dataset that we will use in the correct format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3jUqtLN5P13"},"outputs":[],"source":["def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n","    inputs = next(iter(inputs.take(1)))\n","    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n","    visualization.plot_bounding_box_gallery(\n","        images,\n","        value_range=value_range,\n","        rows=rows,\n","        cols=cols,\n","        y_true=bounding_boxes,\n","        scale=5,\n","        font_scale=0.7,\n","        bounding_box_format=bounding_box_format,\n","        class_mapping=class_mapping,\n","    )\n","\n","\n","def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n","    image = inputs[\"image\"]\n","    boxes = keras_cv.bounding_box.convert_format(\n","        inputs[\"objects\"][\"bbox\"],\n","        images=image,\n","        source=\"rel_yxyx\",\n","        target=bounding_box_format,\n","    )\n","    bounding_boxes = {\n","        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n","        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n","    }\n","    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n","\n","\n","def load_pascal_voc(split, dataset, bounding_box_format):\n","    ds = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n","    ds = ds.map(\n","        lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format),\n","        num_parallel_calls=tf.data.AUTOTUNE,\n","    )\n","    return ds\n","\n","\n","train_ds = load_pascal_voc(\n","    split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\"\n",")\n","eval_ds = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n","\n","train_ds = train_ds.shuffle(BATCH_SIZE * 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blSu6wK15SlP"},"outputs":[],"source":["train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n","eval_ds = eval_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)"]},{"cell_type":"markdown","metadata":{"id":"Wgg85jnOqr4N"},"source":["Viewing some images of the training dataset, to verify their correctness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bGPvJWO5ViK"},"outputs":[],"source":["visualize_dataset(\n","    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"qFkUKFreq6Ag"},"source":["Viewing some images of the evaluation dataset, to verify their correctness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e1HKvRe5YbO"},"outputs":[],"source":["visualize_dataset(\n","    eval_ds,\n","    bounding_box_format=\"xywh\",\n","    value_range=(0, 255),\n","    rows=2,\n","    cols=2,\n","    # If you are not running your experiment on a local machine, you can also\n","    # make `visualize_dataset()` dump the plot to a file using `path`:\n","    # path=\"eval.png\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"AB0Qa0gnrUvd"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"9NQQnqXtrY4R"},"source":["\n","In this section, data augmentation techniques will be used to expand the working dataset. It is important to keep the bounding boxes centered on the correct subjects even if the images are subjected to various distortions, as we see in the printed examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKWOOzWS5anj"},"outputs":[],"source":["augmenter = keras.Sequential(\n","    layers=[\n","        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n","        keras_cv.layers.JitteredResize(\n","            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n","        ),\n","    ]\n",")\n","\n","train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n","visualize_dataset(\n","    train_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBSdPnik5cwY"},"outputs":[],"source":["inference_resizing = keras_cv.layers.Resizing(\n","    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n",")\n","eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8wBF4ZF5dPg"},"outputs":[],"source":["visualize_dataset(\n","    eval_ds, bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"INJJLHfvsj5M"},"source":["In order to be TPU compatible, bounding box Tensors need to be Dense instead of Ragged"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8o6HMTRE5etg"},"outputs":[],"source":["\n","def dict_to_tuple(inputs):\n","    return inputs[\"images\"], bounding_box.to_dense(\n","        inputs[\"bounding_boxes\"], max_boxes=32\n","    )\n","\n","\n","train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"MaJjpzYBtFVy"},"source":["## Optimizer"]},{"cell_type":"markdown","metadata":{"id":"jlDSjDwwtHij"},"source":["We define the starting learning rate which will then be reduced during the process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3N4UOI-G5gma"},"outputs":[],"source":["base_lr = 0.005\n","# including a global_clipnorm is extremely important in object detection tasks\n","optimizer = tf.keras.optimizers.SGD(\n","    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",")"]},{"cell_type":"markdown","metadata":{"id":"NmTVfsVVuMxR"},"source":["## Metrics Evaluation"]},{"cell_type":"markdown","metadata":{"id":"jwslaw4huQUm"},"source":["We will use the COCO metrics, the most widespread, to verify the quality of the training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMl97nkS8KVR"},"outputs":[],"source":["coco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n","    bounding_box_format=\"xywh\", evaluate_freq=20\n",")"]},{"cell_type":"markdown","metadata":{"id":"M_GK1vnDulkl"},"source":["Definition of the function to print these metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTjD8ak85jvI"},"outputs":[],"source":["\n","def print_metrics(metrics):\n","    maxlen = max([len(key) for key in result.keys()])\n","    print(\"Metrics:\")\n","    print(\"-\" * (maxlen + 1))\n","    for k, v in metrics.items():\n","        print(f\"{k.ljust(maxlen+1)}: {v.numpy():0.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"Stk-bKmpu8iA"},"source":["We use a custom callback to ensure model compatibility with TPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3lh40VR5muW"},"outputs":[],"source":["class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n","    def __init__(self, data):\n","        super().__init__()\n","        self.data = data\n","        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n","            bounding_box_format=\"xywh\",\n","            # passing 1e9 ensures we never evaluate until\n","            # `metrics.result(force=True)` is\n","            # called.\n","            evaluate_freq=1e9,\n","        )\n","\n","    def on_epoch_end(self, epoch, logs):\n","        self.metrics.reset_state()\n","        for batch in tqdm.tqdm(self.data):\n","            images, y_true = batch[0], batch[1]\n","            y_pred = self.model.predict(images, verbose=0)\n","            self.metrics.update_state(y_true, y_pred)\n","\n","        metrics = self.metrics.result(force=True)\n","        logs.update(metrics)\n","        return logs"]},{"cell_type":"markdown","metadata":{"id":"nOA6Yf44wIlp"},"source":["## Model Creation"]},{"cell_type":"markdown","metadata":{"id":"VZP3mt9VwSTV"},"source":["We use the pre-trained ResNet50 backbone, from the KerasCV API to construct an untrained RetinaNet Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEMLqDb75o5t"},"outputs":[],"source":["model = keras_cv.models.RetinaNet.from_preset(\n","    \"resnet50_imagenet\",\n","    num_classes=len(class_mapping),\n","    bounding_box_format=\"xywh\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"phPlbP3pwLe5"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"le3Cvy6ewr_T"},"source":["To train our model we use the two functions compile() and fit()"]},{"cell_type":"markdown","metadata":{"id":"h4xaQYqEkkbA"},"source":["Loss Functions\n","\n","\"Focal\" and \"smoothl1\" will be used as loss functions, this is because the first allows us to emphasize more the examples that are carried out with greater difficulty, while the second is used to prevent the learning rate from \"exploding\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHpccqOW5rEd"},"outputs":[],"source":["model.compile(\n","    classification_loss=\"focal\",\n","    box_loss=\"smoothl1\",\n","    optimizer=optimizer,\n","    # We will use our custom callback to evaluate COCO metrics\n","    metrics=None,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhnU0-Q75seQ"},"outputs":[],"source":["model.fit(\n","    train_ds,\n","    validation_data=eval_ds,\n","    # Run for 10-35~ epochs to achieve good scores.\n","    epochs=20,\n","    callbacks=[EvaluateCOCOMetricsCallback(eval_ds)],\n",")"]},{"cell_type":"markdown","metadata":{"id":"wa7Nnpirxe-m"},"source":["## Model Download"]},{"cell_type":"markdown","metadata":{"id":"kfJ_IN4Ixh-2"},"source":["Let's create a folder where we can save the newly trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27iii_Mq5uVD"},"outputs":[],"source":["!mkdir -p saved_model\n","keras.models.save_model(model,'saved_model/my_model', save_format='tf')"]},{"cell_type":"markdown","metadata":{"id":"I-s637Qrxp1z"},"source":["We transform our model into a frozen_graph to be able to use it with OpenCV, using the convert_to_constants() function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DC1B-VZR53o8"},"outputs":[],"source":["from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n","\n","#path of the directory where you want to save your model\n","frozen_out_path = ''\n","\n","# name of the .pb file\n","frozen_graph_filename = 'frozen_graph'\n","\n","#loaded_model = tf.saved_model.load('saved_model/my_model')\n","\n","# Convert Keras model to ConcreteFunction\n","full_model = tf.function(lambda x: model(x))\n","full_model = full_model.get_concrete_function(tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n","\n","# Get frozen ConcreteFunction\n","frozen_func = convert_variables_to_constants_v2(full_model)\n","frozen_func.graph.as_graph_def()\n","layers = [op.name for op in frozen_func.graph.get_operations()]\n","\n","# To print information about the model\n","#print(\"-\" * 60)\n","#print(\"Frozen model layers: \")\n","#for layer in layers:\n","#    print(layer)\n","#print(\"-\" * 60)\n","#print(\"Frozen model inputs: \")\n","#print(frozen_func.inputs)\n","#print(\"Frozen model outputs: \")\n","#print(frozen_func.outputs)\n","# Save frozen graph to disk\n","\n","tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n","                  logdir=frozen_out_path,\n","                  name=f\"{frozen_graph_filename}.pb\",\n","                  as_text=False)\n","# Save its text representation\n","tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n","                  logdir=frozen_out_path,\n","                  name=f\"{frozen_graph_filename}.pbtxt\",\n","                  as_text=True)"]},{"cell_type":"markdown","metadata":{"id":"padPu5dR6QBK"},"source":["## Plotting Results"]},{"cell_type":"markdown","metadata":{"id":"XV5UUee8yRa1"},"source":["We construct a dataset with larger batches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDe7dPle6Pl0"},"outputs":[],"source":["visualization_ds = eval_ds.unbatch()\n","visualization_ds = visualization_ds.ragged_batch(16)\n","visualization_ds = visualization_ds.shuffle(8)"]},{"cell_type":"markdown","metadata":{"id":"ejNGN7NPyhCS"},"source":["We create a function to plot our inferences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NROh7ADx6Rhr"},"outputs":[],"source":["def visualize_detections(model, dataset, bounding_box_format):\n","    images, y_true = next(iter(dataset.take(1)))\n","    y_pred = model.predict(images)\n","    y_pred = bounding_box.to_ragged(y_pred)\n","    visualization.plot_bounding_box_gallery(\n","        images,\n","        value_range=(0, 255),\n","        bounding_box_format=bounding_box_format,\n","        y_true=y_true,\n","        y_pred=y_pred,\n","        scale=4,\n","        rows=2,\n","        cols=4,\n","        show=True,\n","        font_scale=0.7,\n","        class_mapping=class_mapping,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"e-J0sWqFypnN"},"source":["Finally we can visualize our model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCvYfNtt6UiR"},"outputs":[],"source":["model.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n","    bounding_box_format=\"xywh\",\n","    from_logits=True,\n","    iou_threshold=0.5,\n","    confidence_threshold=0.75,\n",")\n","\n","visualize_detections(model, dataset=visualization_ds, bounding_box_format=\"xywh\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMb9TbHY276VVeoczzx61Zi","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
